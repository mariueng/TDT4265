{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Let spatial convolution between an image $I$ and a convolutional kernel $K$ be defined as $I * K$. For the convolved image to be $3 \\times 5$ we add zero-padding to the image $I$. Performing spatial convolution with a stride of $1$ then yields\n",
    " \n",
    "\n",
    "$$\\begin{bmatrix}   0  &  0  &  0  &  0  &  0  &  0  &  0  \\\\\n",
    "                    0  &  1  &  0  &  2  &  3  &  1  &  0  \\\\\n",
    "                    0  &  3  &  2  &  0  &  7  &  0  &  0  \\\\\n",
    "                    0  &  0  &  6  &  1  &  1  &  4  &  0  \\\\\n",
    "                    0  &  0  &  0  &  0  &  0  &  0  &  0  \\\\\n",
    "  \\end{bmatrix}\n",
    "                    *\n",
    "  \\begin{bmatrix}   -1  &  0  &  1   \\\\\n",
    "                    -2  &  0  &  2   \\\\\n",
    "                    -1  &  0  &  1   \\\\\n",
    "  \\end{bmatrix}\n",
    "  =\n",
    "  \\begin{bmatrix}   2  &  -1  &  11  &  -2  &  -13  \\\\\n",
    "                    10  &  -4  &  8  &  2  &  -18  \\\\\n",
    "                    14  &  -1  &  -5  &  6  &  9  \\\\\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## task 1b)\n",
    "\n",
    "Since the weights and biases are the same in the convolutional layer, this means a shift in the input data doesn't affect the \"features\" or feature map. Hence, the (i) Convolutional layer reduces the sensitivity to translationial variations in the input.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Using the provided equations for computing the output shape of convolutional layers\n",
    "\n",
    "\\begin{equation}\n",
    "    W_2 = (W_1 - F_W + 2P_W)/S_W + 1 \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    H_2 = (H_1 - F_H + 2P_H)/S_H + 1 \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "where $H_1 \\times W_1 \\times C_1$ is the shape of the input image, $H_2 \\times W_2 \\times C_2$ is the output shape, $\\bm{F}$ is the receptive field, $\\bm{S}$ is the stride and $\\bm{P}$ is the amount of zero padding applied to the input. Using that we have $H_1 = H_2$, $W_1 = W_2$, $F_W = F_H = 5$, $S_W = S_H = 1$ and solving for $P_W$ and $P_H$ yields\n",
    "\n",
    "P_W &= (5 -1) / 2 = 2\n",
    "P_H &= (5 -1) / 2 = 2\n",
    "\n",
    "\\begin{align}\n",
    "    P_W &= \\frac{1}{2} [(W_1 - 1) * S_W - W_1 + F_W] \\tag{3} \\\\\n",
    "        &= \\frac{1}{2} [(W_1 - 1) * 1 - W_1 + 5] \\\\\n",
    "        &= 2,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    P_H &= \\frac{1}{2} [(H_1 - 1) * S_H - H_1 + F_H] \\tag{4} \\\\\n",
    "        &= \\frac{1}{2} [(H_1 - 1) * 1 - H_1 + 5] \\\\\n",
    "        &= 2.\n",
    "\\end{align}\n",
    "\n",
    "Hence, the amount of padding has to be two in both height and width.\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "Given that $H_1 = W_1 = 512$, $H_2 = W_2 = 504$, $P_W = P_H = 0$, $S_W = S_H = 1$ we solve eq. (1) and eq. (2) for $F_W$ and $F_H$\n",
    "\n",
    "\\begin{align}\n",
    "    F_W &= [W_1 + 2P_W - (W_1 - 1) * S_W] \\tag{5} \\\\\n",
    "        &= [512 + 0 - (504 - 1) * 1] \\\\\n",
    "        &= 9,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    F_H &= [H_1 + 2P_H - (H_2 - 1) * S_H] \\tag{6} \\\\\n",
    "        &= [512 + 0 - (504 - 1) * 1] \\\\\n",
    "        &= 9,\n",
    "\\end{align}\n",
    "\n",
    "respectively. Hence, the shape of the kernels are $9 \\times 9$.\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "Since the task asks about the next layer, I will assume the information from task 1d) is still valid. Thus, using that $H_1 = W_1 = 504$, $P_W = P_H = 0$, $F_W = F_H = 2$ $S_W = S_H = 2$ we solve eq. (1) and eq. (2) for $H_2$ and $W_2$ to get\n",
    "\n",
    "\\begin{align}\n",
    "    W_2 &= (W_1 - F_W + 2P_W)/S_W + 1 \\tag{7} \\\\\n",
    "        &= (504 - 2 + 0) / 2 + 1\\\\\n",
    "        &= 252,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    H_2 &= (H_1 - F_H + 2P_H)/S_H + 1 \\tag{8} \\\\\n",
    "        &= (504 - 2 + 0) / 2 + 1 \\\\\n",
    "        &= 252,\n",
    "\\end{align}\n",
    "\n",
    "respectively. Hence, the shape of the pooled feature maps are $252 \\times 252$.\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "Since the task asks about the next layer, I will assume the information from task 1e) is still valid. Thus, using that $H_1 = W_1 = 252$, $P_W = P_H = 0$, $F_W = F_H = 2$ $S_W = S_H = 2$ we solve eq. (1) and eq. (2) for $H_2$ and $W_2$ to get\n",
    "\n",
    "\\begin{align}\n",
    "    W_2 &= (W_1 - F_W + 2P_W)/S_W + 1 \\tag{9} \\\\\n",
    "        &= (252 - 3 + 0) / 1 + 1\\\\\n",
    "        &= 250,\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "    H_2 &= (H_1 - F_H + 2P_H)/S_H + 1 \\tag{10} \\\\\n",
    "        &= (252 - 3 + 0) / 1 + 1 \\\\\n",
    "        &= 250,\n",
    "\\end{align}\n",
    "\n",
    "respectively. Hence, the shape of the feature maps in the second layer are $250 \\times 250$.\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "We have that for each convolutional layer and pooling layer the number of parameters is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    (F \\times F \\times C_1 + 1) \\times \\text{ \\# of filters } \\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "and zero, respectively. For the fully connected layers, we have that the total number of parameters is given by multyplying the number of weights and biases with the number of hidden units. Hence, we get that the total number of parameters is\n",
    "\n",
    "\\begin{align}\n",
    "    &= \\#params_{layer_1} + \\#params_{layer_2} + \\#params_{layer_3} \\#params_{layer_4} + \\#params_{layer_5} \\tag{12} \\\\\n",
    "    &= (5 \\times 5 \\times 3 + 1) \\times 32 + (5 \\times 5 \\times 32 + 1) \\times 64 + (5 \\times 5 \\times 64 + 1) \\times 128 + (4 \\times 4 \\times 128 + 1) \\times 64 + (64 + 1) \\times 10 \\\\\n",
    "    &= 2,432 + 51,264 + 204,928 + 131,136 + 650 \\\\\n",
    "    &= 390,410.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "![](plots/task2_plot.png)\n",
    "**_Figure 1:_** *Cross entropy loss (left) and accuracy (right) for the CNN defined in Table 1 in the assignment*\n",
    "\n",
    "### Task 2b)\n",
    "Final training, validation and test accuracy was 0.8849, 0.7326 and 0.7336, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "\n",
    "Both models are based on the model from Task 2.\n",
    "\n",
    "#### First network\n",
    "\n",
    "The first network was similar to the original network, except for having twice as many filters as the network in Task 2 and that the Adam optimizer was used instead. There was also added an extra convolution in each convolutional layer. The convolutional layers use 5x5 filters with a stride and padding of size 1 and 2, respectively. It also uses a pooling layers with kernel shapes $2 \\times 2$ and stride of size 2. Batch size is still 64 and early stopping is enabled. Finally, the learning rate used was 0.1.\n",
    "\n",
    "\n",
    "| Layer | Layer Type  | Hidden Units/Filters | Activation |\n",
    "|:-----:|:-----------:|:--------------------:|:----------:|\n",
    "|   1   |   Conv2D    |          64          |    ReLU    |\n",
    "|   1   |   Conv2D    |          64          |    ReLU    |\n",
    "|   1   |  maxPool2D  |          -           |     -      |\n",
    "|   2   |   Conv2D    |         128          |    ReLU    |\n",
    "|   2   |   Conv2D    |         128          |    ReLU    |\n",
    "|   2   |  maxPool2D  |          -           |     -      |\n",
    "|   3   |   Conv2D    |         256          |    ReLU    |\n",
    "|   3   |   Conv2D    |         256          |    ReLU    |\n",
    "|   3   |  maxPool2D  |          -           |     -      |\n",
    "|       |   Flatten   |          -           |     -      |\n",
    "|   4   |     FC      |          64          |     -      |\n",
    "|   5   |     FC      |          10          |     -      |\n",
    "\n",
    "**_Table 1:_** *Architecture of the first network*\n",
    "\n",
    "#### Second network\n",
    "\n",
    "The second network also uses 64 filter and the Adam optimizer with similar hyperparameters as the first model, except for a learning rate of 0.05 instead. In addition, it uses batch normalization as shown in Table 2. \n",
    "\n",
    "| Layer | Layer Type  | Hidden Units/Filters | Activation |\n",
    "|:-----:|:-----------:|:--------------------:|:----------:|\n",
    "|   1   |   Conv2D    |          64          |    ReLU    |\n",
    "|   1   | BatchNorm2D |          -           |     -      |\n",
    "|   1   |  maxPool2D  |          -           |     -      |\n",
    "|   2   |   Conv2D    |         128          |    ReLU    |\n",
    "|   2   | BatchNorm2D |          -           |     -      |\n",
    "|   2   |  maxPool2D  |          -           |     -      |\n",
    "|   3   |   Conv2D    |         256          |    ReLU    |\n",
    "|   3   | BatchNorm2D |          -           |     -      |\n",
    "|   3   |  maxPool2D  |          -           |     -      |\n",
    "|       |   Flatten   |          -           |     -      |\n",
    "|   4   |     FC      |          -           |    ReLU    |\n",
    "|   4   | BatchNorm1D |          -           |     -      |\n",
    "|   5   |     FC      |          -           |     -      |\n",
    "\n",
    "\n",
    "**_Table 2:_** *Architecture of the second network*\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "#### First model\n",
    "\n",
    "Performance of first model:\n",
    "\n",
    "- Training accuracy: 0.9047\n",
    "- Training loss: 0.2845\n",
    "- Validation accuracy: 0.7516\n",
    "- Test accuracy: 0.7552\n",
    "\n",
    "#### Second model \n",
    "\n",
    "Performance of second model:\n",
    "\n",
    "- Training accuracy: 0.9600\n",
    "- Training loss: 0.1293\n",
    "- Validation accuracy: 0.7644\n",
    "- Test accuracy: 0.7596\n",
    "\n",
    "#### Plot of the best model\n",
    "\n",
    "![](plots/task3_model_one_plot.png)\n",
    "**_Figure 2:_** *Cross entropy loss (left) and accuracy (right) for the best model, i.e. the second model*\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "The best improvements by far was seen when applying Batch Normalization. Not only did this increase the convergence speed, but it also increased the performance significantly. There are different reasonings trying to explain why batch normalization works. Some argue that the increase in convergence speed is due to the noramlizing inputs between similar ranges. In the original paper, it is argued that the batch normalization reduces the internal covariate shift of the network, which is a shift in the data distribution. Finally, it is believed that batch normalization also has a regularization effect.\n",
    "\n",
    "Increasing the number of filters from 32 to 64 also improved the results. This is most likely due to the fact that more filters was able to capture more patterns as the layers become increasingly complex. However, one should be careful to not use too many filters such that non-existent patterns in the original image are captured by a overcomplex model.\n",
    "\n",
    "In the beginning I tried to experiment with different optimizers than SGD, such as Adam. However, simply changing the optimizer without tuning the parameters resulted in poor performance. This is most likely due to the fact that the hyperparameters needed to be tuned. Adam still had faster convergence compared to the SGD. When I added L2 regularization in the Adam optimizer it performed similarly to the SGD optimizer without L2 regularization. When adding L2 regularization to the SGD optimizer the first network performed better, but the second one was slightly worse. I believe the reason for this is that the L2 regularization is different.\n",
    "\n",
    "Other attempts that failed to improve the performance was different activation functions, larger filters and changing pooling layers. Smaller filters worked similarly to the to the ones from Task 2.\n",
    "\n",
    "### Task 3d)\n",
    "\n",
    "![](plots/Task3d_technique_plot.png)\n",
    "**_Figure 3:_** *Cross entropy and validation loss for the network with (blue and orange) and without (green and red) batch normalization*\n",
    "\n",
    "### Task 3e)\n",
    "\n",
    "To obtain a higher performance the following changes was made to the second model:\n",
    "- Change back to SGD optimiser with a learning rate of 0.05\n",
    "- Added more convoloutional layers, specifically doubling them in each layer.\n",
    "\n",
    "The best model ended up with the following performance:\n",
    "\n",
    "- Training accuracy: 0.9707\n",
    "- Training loss: 0.0932\n",
    "- Validation accuracy: 0.8121\n",
    "- Test accuracy: 0.8042\n",
    "\n",
    "![](plots/task3_best_model_plot.png)\n",
    "**_Figure 4:_** *Cross entropy loss (left) and accuracy (right) for the best network with over 80% accuracy on the test set*\n",
    "\n",
    "### Task 3f)\n",
    "Yes. We can clearly see that the best model is overfitting. This most likely due to the model being to complex and attempting to fit to the training data, at the cost of not representing the test data well. It could also be the case that the test set is to similar to the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "The Resnet18 model was implemented with the following hyperparameters:\n",
    "- Optimizer: Adam\n",
    "- Batch size: 32\n",
    "- Learning rate: $5 * 10^{-4}$\n",
    "- No data augmentation.\n",
    "\n",
    "The final test accuracy was $0.8937$.\n",
    "\n",
    "![](plots/Task4a_resnet18_plot.png)\n",
    "**_Figure 5:_** *Taining and validation loss for the Resnet18 model*\n",
    "\n",
    "## Task 4b)\n",
    "\n",
    "Figure 6 visualizes the filters from the first convolutional layer of the trained model by passing an image through the filter of the resnet18 network. What we can see is that it looks like each filter extracts different patterns from the original image. For instance we see that filter 32 and 52 have a high range on the activation functions in opposite signs (positive and negative of image) yielding high contrasts. Furtehrmore we see that if we loo closely at the clouds and zebra stripes that filter 14 activates on the vertical lines whereas filter 26 activates on the horizontal lines. \n",
    "\n",
    "![](plots/Task4b.png)\n",
    "**_Figure 6:_** *The filter weights (top row) and the activation to the corresponding filter (bottom row) of the first convolutional layer in ResNet18. This is visualized for the following indices: [14, 26, 32, 49, 52]*\n",
    "\n",
    "## Task 4c)\n",
    "\n",
    "Figure 7 shows the 10 first filters in the last convolutional layer. Since this layer is close to the actual ouput one could imagine that the network at this point attempts to identify the most important patterns of the image, i.e. a zebra. While more abstract, one could imagine that the filters illustrate important parts of the zebra body such as the stripes, shape of legs, body and so on.\n",
    "\n",
    "![](plots/Task4c.png)\n",
    "**_Figure 7:_** *The 10 first filters in the last convolutional layer of the resnet18 network.*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
