{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report\n",
    "\n",
    "## Group 69\n",
    "\n",
    "### Authors: Marius Engen & Sindre Ã˜versveen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We are given that the gradient descent rule for the hidden layer is\n",
    "\n",
    "\\begin{equation}\n",
    "    w_{ji} := w_{ji} - \\alpha \\frac{\\partial C }{\\partial w_{ji}}, \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "and we know that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_{k} = \\frac{\\partial C }{\\partial z_{k}} = - (y_{k} - \\hat{y}_{k}), \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{j} = f(z_{j}), \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    z_{j} = \\sum_{i = 0}^{I} w_{ji} x_i, \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    z_{k} = \\sum_{j = 0}^{J} w_{kj} a_j. \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "First, we use the chain rule to rewrite the partial derivative in eq. (1)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C }{\\partial w_{ji}} = \\sum_{k} \\frac{\\partial C }{\\partial z_{k}} \\frac{\\partial z_{k} }{\\partial a_{j}} \\frac{\\partial a_{j} }{\\partial z_{j}} \\frac{\\partial z_{j} }{\\partial w_{ji}}. \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "Then, we substitute the first partial derivative with $\\delta_{k}$ using eq. (2), $a_j$ using eq. (3), $z_k$ using eq. (3) and $z_j$ using eq. (4) to get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C }{\\partial w_{ji}} &= \\sum_{k} \\delta_{k} \\frac{\\partial }{\\partial a_{j}} \\left[ \\sum_{j = 0}^{J} w_{kj} a_j \\right] \\frac{\\partial }{\\partial z_{j}} f(z_j) \\frac{\\partial }{\\partial w_{ji}} \\left[ \\sum_{i = 0}^{I} w_{ji} x_i \\right], \\tag{7} \\\\\n",
    "    &= \\sum_{k} \\delta_{k} w_{kj} f'(z_j) x_i.\n",
    "\\end{align}\n",
    "\n",
    "Rearringing the terms while defining $\\delta_{j} = f'(z) \\sum_{k} w_{kj} \\delta_{k}$ and inserting this back into eq. (1) yields\n",
    "\n",
    "\\begin{equation}\n",
    "    w_{ji} := w_{ji} - \\alpha \\delta_{j} x_i, \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_{j} = f'(z) \\sum_{k} w_{kj} \\delta_{j}. \\tag{9}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Let $N$ and $K$ be the number of data points and outputs (labels or classes), respectively. $I$ and $J$ are the number of nodes in the input and hidden layer, respectively. We then define the following notation\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{X} &: (N \\times I) &\\text{ input nodes matrix}  \\tag{10} \\\\\n",
    "    \\bm{W}_{ji} &: (I \\times J) &\\text{ weight matrix between input and hidden layer} \\\\\n",
    "    \\bm{W}_{kj} &: (J \\times K) &\\text{ weight matrix between hidden and output layer} \\\\\n",
    "    \\bm{Z}_{j} &: (N \\times J) &\\text{ hidden unit input matrix} \\\\\n",
    "    \\bm{\\delta}_{j} &: (N \\times J) &\\text{ error matrix in the hidden layer} \\\\\n",
    "    \\bm{\\delta}_{k} &: (N \\times K) &\\text{ error matrix in the output layer} \\\\\n",
    "    \\bm{A}_{j} &: (N \\times J) &\\text{ activation matrix in layer j}\n",
    "\\end{align}\n",
    "\n",
    "We can then rewrite the error $\\delta_{j}$ as a vector using the hadamard product to perform elementwise multiplication between a vector and a matrix\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{\\delta}_{j} = \\bm{f}(\\bm{Z}_{j}) \\odot \\delta_{k} \\bm{W}_{kj}^\\top. \\tag{11}\n",
    "\\end{align}\n",
    "\n",
    "Vectorizing the update rule between the input and hidden layer, eq. (8), then becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{W}_{ji} := \\bm{W}_{ji} - \\alpha \\bm{X}^\\top \\bm{\\delta}_{j}. \\tag{12}\n",
    "\\end{align}\n",
    "\n",
    "The update rule for between the output and hidden layer is written as\n",
    "\n",
    "\\begin{align}\n",
    "    w_{kj} := w_{kj} - \\alpha a_{j} \\delta_{k}. \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the vectorized update rule between the output and hidden layer becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{W}_{kj} := \\bm{W}_{kj} - \\alpha A_{j}^\\top \\bm{\\delta}_{k}. \\tag{14}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "\n",
    "Training set mean: 33.55274553571429\n",
    "\n",
    "Training set standard deviation: 78.87550070784701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right)\"](img/task2/task2c_train_loss.png)\n",
    "**_Figure 1:_** *Training cross entropy loss (left) and validation accuracy (right)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The model in task 2c) has 785 input nodes, 64 nodes in the hidden layer, and 10 output nodes. Since the biases in this model only are included in the input nodes using the bias trick (constant 1 instead of a variable), the total number of parameters in the model is\n",
    "\n",
    "\\begin{equation*}\n",
    "    785 \\times 64 + 64 \\times 10 = 50,880.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we've defined the incremental models as follows:\n",
    "\n",
    "* **Base model**: No improvements, same model as in task 2\n",
    "* **Model a**: Using improved weight initialization\n",
    "* **Model b**: Using improved weight initalization and improved sigmoid activation funciton\n",
    "* **Model c**: Using improved weight initialization, improved sigmoid activation function and momentum\n",
    "\n",
    "*_val and train in figure legends are short for validation and training, respectively._\n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for base model (blue) and model a (orange)\"](img/task3/Task3a_base_model_vs_model_a.png)\n",
    "**_Figure 2:_** *Averaged cross entropy loss (left) and accuracy (right) for base model training set (blue), base model validation set (orange), model a training set (green) and model a validation set (red)*\n",
    "\n",
    "Figure 2 shows a clear improvement in both convergence speed and accuracy. This means that the use of the normally distributed weights with mean zero allows for finding better optima faster, which is why the training cross entropy loss is lower earlier for **model a** than the **base model**. In terms of generalization, it looks like there is a slight increase in the cross entropy loss for **model a** towards the end. This is a slight indication that **model a** is overfitting, hence a reduced generalization.\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for model a (blue) and model b (orange)\"](img/task3/Task3b_model_a_vs_model_b.png)\n",
    "**_Figure 3:_** *Averaged cross entropy loss (left) and accuracy (right) for model a training set (blue), model a validation set (orange), model b training set (green) and model b validation set (red)*\n",
    "\n",
    "Figure 3 again shows a slight increase in convergence speed and accuracy by using the improved sigmoid function. However, now **model b** is definitely overfitting. This can be seen by the upwards curve in the validation cross entropy loss for **model b** (red). This is also confirmed by the accuracy plot (right), where we see that the training accuracy for **model b** has reached almost 100%, while the validation error is almost deacreasing with the number of steps.\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for b (blue) and model c (orange)\"](img/task3/Task3c_model_b_vs_model_c.png)\n",
    "**_Figure 4:_** *Averaged cross entropy loss (left) and accuracy (right) for model b training set (blue), model b validation set (orange), model c training set (green) and model c validation set (red)*\n",
    "\n",
    "Figure 4 shows that adding the momentum does not seem to provide a significant improvement in either convergence speed or accuracy. If anything, it is doing the opposite as the averaged cross entropy loss for **model c** on the training set (left, red) is worse than the equivalent of **model b** (left, orange). This means there is no improvement in terms of generalization, if any, a deteriation which results in a more overfitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a), b)\n",
    "\n",
    "In Figure 5 we see the result of using 32, 64 and 128 units in the hidden layer for the validation loss. We see that when the number of hidden units is too small, the network converges to a worse local optima resulting in worse accuracy compared with the network with 64 hidden units. There is no appearent difference in convergence speed, although computationally the network with 32 hidden units is significantly faster than the others. The reason for the higher loss might be that the network is too simple to capture the underlying distribution of the data. The network with 128 hidden units has a significantly lower loss and higher computational time than the other two networks. Hence, there is a tradeoff between computational time and model complexity. It should be notede that the network with 128 neurons seems to be better in avoiding overfitting than the network with 32 units due to the slope of the converged loss. \n",
    "\n",
    "![\"\"](img/task4/Task4ab_validation_loss.png)\n",
    "**_Figure 5:_** *Validation loss for networks with 32 (blue), 64 (orange) and 128 (green) hidden units.*\n",
    "<!-- ![\"\"](img/task4/Task4ab_validation_accuracy.png)\n",
    "**_Figure 5:_** *Validation accuracy for networks with 32 (blue), 64 (orange), 128 (green) hidden units.* -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "To create a model with two hidden layers and equal number of parameters as the network in task 3, we know that the total number of parameters in a network can be expressed as\n",
    "\n",
    "\\begin{equation}\n",
    "    parameters_{model} = |w| + |b|, \\tag{15}\n",
    "\\end{equation}\n",
    "\n",
    "where $|w|$ and $|b|$ is the number of weights and biases, respectively. Furthermore, the network in Task 3 was the same structure as in Task 2d), hence it had 50,880 parameters. Since we are given that the new network should have 2 hidden layers with an equal amount of hidden units, lets define this number as $x$, we can write the number of parameters in the new model as\n",
    "\n",
    "\\begin{equation}\n",
    "    parameters_{new} = 785 \\times x + x \\times x + x \\times 10. \\tag{16}\n",
    "\\end{equation}\n",
    "\n",
    "Setting eq. (16) to the same number of parameters as the network in Task 3 and solving for $x$ yields\n",
    "\n",
    "\\begin{align}\n",
    "    785 \\times x + x \\times x + x \\times 10 &= 50,880 \\tag{17} \\\\\n",
    "    x^2 + 795x &= 50,880,\n",
    "\\end{align}\n",
    "\n",
    "which gives\n",
    "\n",
    "\\begin{equation}\n",
    "    x = - \\frac{795}{2} - \\frac{\\sqrt{835,545}}{2} \\vee x = \\frac{\\sqrt{835,545}}{2} - \\frac{795}{2}. \\tag{18}\n",
    "\\end{equation}\n",
    "\n",
    "The negative answer is not feasible, hence the number of hidden units in the new network to approximately have the same number of paramaters as the network in Task 3 becomes $x \\approx 60$ giving a total of $51,300$ parameters.\n",
    "\n",
    "In Figure 6 and Figure 7 we see the comparison of the network with a single and multiple hidden layers in terms of loss and accuracy. Compared to the network in Task 3 it performs similarly, perhaps slightly worse. While the network is overfitting less, it is more computationally heavy and is therefore probably not preferred over the the network in Task 3. \n",
    "\n",
    "![\"\"](img/task4/Task4d_accuracy.png)\n",
    "**_Figure 6:_** *Validation loss for networks with 32 (blue), 64 (orange) and 128 (green) hidden units.*\n",
    "![\"\"](img/task4/Task4d_loss.png)\n",
    "**_Figure 7:_** *Validation loss for networks with 32 (blue), 64 (orange) and 128 (green) hidden units.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "In Figure 8 and 9 we see the loss and accuracy of a network with 10 hidden layers each with 64 hidden units. Compared to the network with two hidden layers in the previous task, this network is too complex and overfits the underlying distribution. This can be seen by the increased variance in the loss and the dips in accuracy. The reason for this is that the network now is sensitive to small changes in the input, resulting in large changes in the ouputs giving us a spiky pattern see in both figures. This ultimately also gives a slower convergence, since the network is trying to fit a overcomplicated model to the underlying distribution. \n",
    "\n",
    "![\"\"](img/task4/Task4e_accuracy.png)\n",
    "**_Figure 8:_** *Validation loss for networks with 10 hidden layers.*\n",
    "![\"\"](img/task4/Task4e_loss.png)\n",
    "**_Figure 9:_** *Validation loss for networks with 10 hidden layers.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
