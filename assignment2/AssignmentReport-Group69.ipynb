{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Report\n",
    "\n",
    "## Group 69\n",
    "\n",
    "### Authors: Marius Engen & Sindre Ã˜versveen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We are given that the gradient descent rule for the hidden layer is\n",
    "\n",
    "\\begin{equation}\n",
    "    w_{ji} := w_{ji} - \\alpha \\frac{\\partial C }{\\partial w_{ji}}, \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "and we know that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_{k} = \\frac{\\partial C }{\\partial z_{k}} = - (y_{k} - \\hat{y}_{k}), \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    a_{j} = f(z_{j}), \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    z_{j} = \\sum_{i = 0}^{I} w_{ji} x_i, \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "    z_{k} = \\sum_{j = 0}^{J} w_{kj} a_j. \\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "First, we use the chain rule to rewrite the partial derivative in eq. (1)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C }{\\partial w_{ji}} = \\sum_{k} \\frac{\\partial C }{\\partial z_{k}} \\frac{\\partial z_{k} }{\\partial a_{j}} \\frac{\\partial a_{j} }{\\partial z_{j}} \\frac{\\partial z_{j} }{\\partial w_{ji}}. \\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "Then, we substitute the first partial derivative with $\\delta_{k}$ using eq. (2), $a_j$ using eq. (3), $z_k$ using eq. (3) and $z_j$ using eq. (4) to get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C }{\\partial w_{ji}} &= \\sum_{k} \\delta_{k} \\frac{\\partial }{\\partial a_{j}} \\left[ \\sum_{j = 0}^{J} w_{kj} a_j \\right] \\frac{\\partial }{\\partial z_{j}} f(z_j) \\frac{\\partial }{\\partial w_{ji}} \\left[ \\sum_{i = 0}^{I} w_{ji} x_i \\right], \\tag{7} \\\\\n",
    "    &= \\sum_{k} \\delta_{k} w_{kj} f'(z_j) x_i.\n",
    "\\end{align}\n",
    "\n",
    "Rearringing the terms while defining $\\delta_{j} = f'(z) \\sum_{k} w_{kj} \\delta_{k}$ and inserting this back into eq. (1) yields\n",
    "\n",
    "\\begin{equation}\n",
    "    w_{ji} := w_{ji} - \\alpha \\delta_{j} x_i, \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_{j} = f'(z) \\sum_{k} w_{kj} \\delta_{j}. \\tag{9}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "Let $N$ and $K$ be the number of data points and outputs (labels or classes), respectively. $I$ and $J$ are the number of nodes in the input and hidden layer, respectively. We then define the following notation\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{X} &: (N \\times I) &\\text{ input nodes matrix}  \\tag{10} \\\\\n",
    "    \\bm{W}_{ji} &: (I \\times J) &\\text{ weight matrix between input and hidden layer} \\\\\n",
    "    \\bm{W}_{kj} &: (J \\times K) &\\text{ weight matrix between hidden and output layer} \\\\\n",
    "    \\bm{Z}_{j} &: (N \\times J) &\\text{ hidden unit input matrix} \\\\\n",
    "    \\bm{\\delta}_{j} &: (N \\times J) &\\text{ error matrix in the hidden layer} \\\\\n",
    "    \\bm{\\delta}_{k} &: (N \\times K) &\\text{ error matrix in the output layer} \\\\\n",
    "    \\bm{A}_{j} &: (N \\times J) &\\text{ activation matrix in layer j}\n",
    "\\end{align}\n",
    "\n",
    "We can then rewrite the error $\\delta_{j}$ as a vector using the hadamard product to perform elementwise multiplication between a vector and a matrix\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{\\delta}_{j} = \\bm{f}(\\bm{Z}_{j}) \\odot \\delta_{k} \\bm{W}_{kj}^\\top. \\tag{11}\n",
    "\\end{align}\n",
    "\n",
    "Vectorizing the update rule between the input and hidden layer, eq. (8), then becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{W}_{ji} := \\bm{W}_{ji} - \\alpha \\bm{X}^\\top \\bm{\\delta}_{j}. \\tag{12}\n",
    "\\end{align}\n",
    "\n",
    "The update rule for between the output and hidden layer is written as\n",
    "\n",
    "\\begin{align}\n",
    "    w_{kj} := w_{kj} - \\alpha a_{j} \\delta_{k}. \\tag{13}\n",
    "\\end{align}\n",
    "\n",
    "Hence, the vectorized update rule between the output and hidden layer becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\bm{W}_{kj} := \\bm{W}_{kj} - \\alpha A_{j}^\\top \\bm{\\delta}_{k}. \\tag{14}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "\n",
    "Training set mean: 33.55274553571429\n",
    "\n",
    "Training set standard deviation: 78.87550070784701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](img/task2/task2c_train_loss.png)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right)\"]((img/task2/task2c_train_loss.png)\n",
    "**_Figure 1:_** *Training cross entropy loss (left) and validation accuracy (right)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "The model in task 2c) has 785 input nodes, 64 nodes in the hidden layer, and 10 output nodes. Since the biases in this model only are included in the input nodes using the bias trick (constant 1 instead of a variable), the total number of parameters in the model is\n",
    "\n",
    "\\begin{equation*}\n",
    "    785 \\times 64 + 64 \\times 10 = 50,880.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we've defined the incremental models as follows:\n",
    "\n",
    "* **Base model**: No improvements, same model as in task 2\n",
    "* **Model a**: Using improved weight initialization\n",
    "* **Model b**: Using improved weight initalization and improved sigmoid activation funciton\n",
    "* **Model c**: Using improved weight initialization, improved sigmoid activation function and momentum\n",
    "\n",
    "*_val and train in figure legends are short for validation and training, respectively._\n",
    "\n",
    "### Task 3a)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for base model (blue) and model a (orange)\"](img/task3/Task3a_all_base_model_vs_model_a.png)\n",
    "**_Figure 2:_** *Averaged cross entropy loss (left) and accuracy (right) for base model training set (blue), base model validation set (orange), model a training set (green) and model a validation set (red)*\n",
    "\n",
    "Figure 2 shows a clear improvement in both convergence speed and accuracy. This means that the use of the normally distributed weights with mean zero allows for finding better optima faster, which is why the training cross entropy loss is lower earlier for **model a** than the **base model**. In terms of generalization, it looks like there is a slight increase in the cross entropy loss for **model a** towards the end. This is a slight indication that **model a** is overfitting, hence a reduced generalization.\n",
    "\n",
    "### Task 3b)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for model a (blue) and model b (orange)\"](img/task3/Task3b_all_model_a_vs_model_b.png)\n",
    "**_Figure 3:_** *Averaged cross entropy loss (left) and accuracy (right) for model a training set (blue), model a validation set (orange), model b training set (green) and model b validation set (red)*\n",
    "\n",
    "Figure 3 again shows a slight increase in convergence speed and accuracy by using the improved sigmoid function. However, now **model b** is definitely overfitting. This can be seen by the upwards curve in the validation cross entropy loss for **model b** (red). This is also confirmed by the accuracy plot (right), where we see that the training accuracy for **model b** has reached almost 100%, while the validation error is almost deacreasing with the number of steps.\n",
    "\n",
    "### Task 3c)\n",
    "\n",
    "![\"Training cross entropy loss (left) and validation accuracy (right) for b (blue) and model c (orange)\"](img/task3/task3c_all_model_b_vs_model_c.png)\n",
    "**_Figure 4:_** *Averaged cross entropy loss (left) and accuracy (right) for model b training set (blue), model b validation set (orange), model c training set (green) and model c validation set (red)*\n",
    "\n",
    "Figure 4 shows that adding the momentum does not seem to provide a significant improvement in either convergence speed or accuracy. If anything, it is doing the opposite as the averaged cross entropy loss for **model c** on the training set (left, red) is worse than the equivalent of **model b** (left, orange). This means there is no improvement in terms of generalization, if any, a deteriation which results in a more overfitted model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "fkdsafds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](img/task4/task4ab_validation_loss.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
