{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "From Equation 3 in the assignment we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    C^{n}(w) = -(y^{n} ln(\\hat{y}^{n}) + (1 - y^{n}) ln(1 - \\hat{y}^{n})). \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We also know that the derivative of the natural logarithm is given as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial}{\\partial x} \\ln{x} = \\frac{1}{x}, \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "and we are given the derivative of the sigmoid function as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial}{\\partial w_{i}} f(x^{n}) = x_{i}^{n} f(x^n) (1 - f(x^n)). \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivative of eq. (1) with respect to a single weight $ w_{i} $ then becomes\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= \\frac{\\partial}{\\partial w_{i}} (-(y^{n} \\ln{\\hat{y}^{n}} + (1 - y^{n}) \\ln{(1 - \\hat{y}^{n})})), \\tag{4} \\\\\n",
    "  &= - y^{n} \\frac{\\partial}{\\partial w_{i}} \\ln{\\hat{y}^{n}} - (1 - y^{n}) \\frac{\\partial}{\\partial w_{i}} \\ln{(1 - \\hat{y}^{n})}.\n",
    "\\end{align}\n",
    "\n",
    "Substituting $ \\hat{y}^n = f(x^n) $ while combining eq. (2) with the chain rule yields\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} \\frac{\\partial}{\\partial w_{i}} \\ln{(f(x^n)) - (1 - y^{n})} \\frac{\\partial}{\\partial w_{i}} \\ln{(1 - f(x^n))} \\tag{5} \\\\\n",
    "  &= - y^{n} \\frac{1}{f(x^n)} \\frac{\\partial}{\\partial w_{i}} f(x^n) - (1 - y^{n}) \\frac{1}{1 - f(x^n)} \\frac{\\partial}{\\partial w_{i}} (1 - f(x^n)).\n",
    "\\end{align}\n",
    "\n",
    "We then utilize the derivative given in eq. (3) to obtain\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} \\frac{1}{f(x^n)} x_{i}^{n} f(x^n) (1 - f(x^n)) + (1 - y^{n}) \\frac{1}{1 - f(x^n)} x_{i}^{n} f(x^n) (1 - f(x^n)) \\tag{6} \\\\\n",
    "  &= - y^{n} x_{i}^{n} (1 - f(x^n)) + (1 - y^{n}) x_{i}^{n} f(x^n).\n",
    "\\end{align}\n",
    "\n",
    "Substituting back $ f(x^n) = \\hat{y}^n $ and rearranging the terms shows that the gradient for eq. (3) in the assignment is given by\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} x_{i}^{n} (1 - \\hat{y}^n) + (1 - y^{n}) x_{i}^{n} \\hat{y}^n \\tag{7} \\\\\n",
    "  &= y^{n} x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} + x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} \\hat{y}^n \\\\\n",
    "  &= x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} \\\\\n",
    "  &= - (y^{n} - \\hat{y}^n) x_{i}^{n}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "\n",
    "From Equation 5 in the assignment we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    C^{n}(w) = - \\sum_{k=1}^{K} y_{k}^{n} ln(\\hat{y}_{k}^{n}). \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "We also know that the softmax function is written\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{y}_{k} = \\frac{e^{z_k}}{\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}}, \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "and we are given that\n",
    "\n",
    "\\begin{equation}\n",
    "  \\sum_{k = 1}^{K} y_{k}^{n} = 1, \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "  ln(\\frac{a}{b}) = \\ln{a} - \\ln{b}. \\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivative of eq. (8) with respect to a single weight $ w_{k j} $ then becomes\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} = \\frac{\\partial }{\\partial w_{kj}} \\left[ - \\sum_{k=1}^{K} y_{k}^{n} \\ln{(\\hat{y}_{k}^{n})} \\right], \\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "where we see that for since the partial derivate is with respect to $w_{k j}$ all the other weights will be constants, i.e. their derivate is zero, so we can remove the summation\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} = - y_{k}^{n} \\frac{\\partial }{\\partial w_{kj}} \\ln{(\\hat{y}_{k}^{n})}. \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "This means we're left with derivating the natural logarithm of the softmax function. Using eq. (2) with the chain rule yields\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} = - y_{k}^{n} \\frac{1}{\\hat{y}_{k}^{n}} \\frac{\\partial \\hat{y}_{k}^{n}}{\\partial w_{kj}} . \\tag{14}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Substituting $\\hat{y}_{k}^{n}$ using eq. (9) then gives us\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} &= \\frac{\\partial }{\\partial w_{kj}} \\ln{ \\left( \\frac{e^{z_k}}{\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}} \\right) }, \\tag{13} \\\\\n",
    "  &= \\frac{\\partial }{\\partial w_{kj}} \\ln{(e^{z_k})} - \\ln{(\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}})}\n",
    "\\end{align}\n",
    "\n",
    "where we use the formula of the derivate of the natural logarithm from eq. (2) to get\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}}  = \\frac{1}{\\hat{y}_{k}^{n}} \\frac{\\partial }{\\partial w_{kj}} \\hat{y}_{k}^{n}, \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}}  = \\frac{\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}}{e^{z_k}} \\frac{\\partial }{\\partial w_{kj}} \\frac{e^{z_k}}{\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}}. \\tag{14}\n",
    "\\end{align}\n",
    "\n",
    "Here, we apply the quotiont rule of derivatives using that\n",
    "\n",
    "\\begin{align}\n",
    "  g_{k} &= e^{z_{k}}, \\tag{15} \\\\\n",
    "  h_{k} &= \\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}.\n",
    "\\end{align}\n",
    "\n",
    "Now, the derivative of $h_k$ will always be $e^{z_k}$ for whatever weight $w_{k j}$ we use choose. However, for $ g_{k} $ this is not the case. Here we have that the derivative of $g_{k}$ with respect to $w_{k j}$ equals $e^{z_k}$ only if $n = k$. Otherwise, the derivative of $g_{k}$ is zero.\n",
    "\n",
    "\\begin{align}\n",
    "  \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "FILL IN ANSWER\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "FILL IN ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "FILL IN ANSWER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
