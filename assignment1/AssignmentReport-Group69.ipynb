{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a)\n",
    "\n",
    "From Equation 3 in the assignment we have that \n",
    "\n",
    "\\begin{equation}\n",
    "    C^{n}(w) = -(y^{n} ln(\\hat{y}^{n}) + (1 - y^{n}) ln(1 - \\hat{y}^{n})). \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We also know that the derivative of the natural logarithm is given as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial}{\\partial x} \\ln{x} = \\frac{1}{x}, \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "and we are given the derivative of the sigmoid function as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial}{\\partial w_{i}} f(x^{n}) = x_{i}^{n} f(x^n) (1 - f(x^n)). \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivative of eq. (1) with respect to a single weight $ w_{i} $ then becomes\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= \\frac{\\partial}{\\partial w_{i}} (-(y^{n} \\ln{\\hat{y}^{n}} + (1 - y^{n}) \\ln{(1 - \\hat{y}^{n})})), \\tag{4} \\\\\n",
    "  &= - y^{n} \\frac{\\partial}{\\partial w_{i}} \\ln{\\hat{y}^{n}} - (1 - y^{n}) \\frac{\\partial}{\\partial w_{i}} \\ln{(1 - \\hat{y}^{n})}.\n",
    "\\end{align}\n",
    "\n",
    "Substituting $ \\hat{y}^n = f(x^n) $ while combining eq. (2) with the chain rule yields\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} \\frac{\\partial}{\\partial w_{i}} \\ln{(f(x^n)) - (1 - y^{n})} \\frac{\\partial}{\\partial w_{i}} \\ln{(1 - f(x^n))} \\tag{5} \\\\\n",
    "  &= - y^{n} \\frac{1}{f(x^n)} \\frac{\\partial}{\\partial w_{i}} f(x^n) - (1 - y^{n}) \\frac{1}{1 - f(x^n)} \\frac{\\partial}{\\partial w_{i}} (1 - f(x^n)).\n",
    "\\end{align}\n",
    "\n",
    "We then utilize the derivative given in eq. (3) to obtain\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} \\frac{1}{f(x^n)} x_{i}^{n} f(x^n) (1 - f(x^n)) + (1 - y^{n}) \\frac{1}{1 - f(x^n)} x_{i}^{n} f(x^n) (1 - f(x^n)) \\tag{6} \\\\\n",
    "  &= - y^{n} x_{i}^{n} (1 - f(x^n)) + (1 - y^{n}) x_{i}^{n} f(x^n).\n",
    "\\end{align}\n",
    "\n",
    "Substituting back $ f(x^n) = \\hat{y}^n $ and rearranging the terms shows that the gradient for eq. (3) in the assignment is given by\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{i}} &= - y^{n} x_{i}^{n} (1 - \\hat{y}^n) + (1 - y^{n}) x_{i}^{n} \\hat{y}^n \\tag{7} \\\\\n",
    "  &= y^{n} x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} + x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} \\hat{y}^n \\\\\n",
    "  &= x_{i}^{n} \\hat{y}^n - y^{n} x_{i}^{n} \\\\\n",
    "  &= - (y^{n} - \\hat{y}^n) x_{i}^{n}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b)\n",
    "\n",
    "From Equation 5 in the assignment we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    C^{n}(w) = - \\sum_{k=1}^{K} y_{k}^{n} ln(\\hat{y}_{k}^{n}). \\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "We also know that the softmax function is written\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat{y}_{k} = \\frac{e^{z_k}}{\\sum_{k^{'}}^{K}{e^{z_{k^{'}}}}}, \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "and we are given that\n",
    "\n",
    "\\begin{equation}\n",
    "  \\sum_{k = 1}^{K} y_{k}^{n} = 1, \\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "  ln(\\frac{a}{b}) = \\ln{a} - \\ln{b}. \\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "The partial derivative of eq. (8) with respect to a single weight $ w_{k j} $ then becomes\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} = \\frac{\\partial }{\\partial w_{kj}} \\left[ - \\sum_{k=1}^{K} y_{k}^{n} \\ln{(\\hat{y}_{k}^{n})} \\right]. \\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "Inserting eq. (9) and writing out the terms using eq. (11) yields\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} = - \\frac{\\partial \\hat{y}_{k}^{n}}{\\partial w_{kj}} \\left[ \\sum_{k=1}^{K} y_{k}^{n} w_{k}^T x^n \\right] + \\frac{\\partial }{\\partial w_{kj}} \\left[ \\sum_{k=1}^{K} y_{k}^{n} \\log{\\left( \\sum_{k^{'}}^{K}{e^{w_{k'}^T x^n}} \\right)} \\right]. \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "The first term of eq. (13) becomes\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial \\hat{y}_{k}^{n}}{\\partial w_{kj}} \\left[ \\sum_{k=1}^{K} y_{k}^{n} w_{k}^T x^n \\right] = y_{k}^{n} x_{j}^{n}. \\tag{14}\n",
    "\\end{equation}\n",
    "\n",
    "Now, the partial derivative of the second term for $k' \\neq k$ are all zero, since they are constants. The partial derivative of the second term therefore becomes \n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial }{\\partial w_{k j}} \\left[ \\sum_{k=1}^{K} y_{k}^{n} \\log{\\left( \\sum_{k^{'}}^{K}{w_{k}^T x^n} \\right)} \\right] &= \\sum_{k=1}^{K} y_{k}^{n}  \\frac{\\partial }{\\partial w_{k j}} \\left[ \\log{\\left( \\sum_{k^{'}}^{K}{w_{k}^T x^n} \\right)} \\right] \\tag{15} \\\\\n",
    "  &= \\frac{e^{w_{k}^T x^n}}{\\sum_{k'}^{K} e^{w_{k'}^T x^n}} x_{j}^{n} \\sum_{k=1}^{K} y_{k}^{n},\n",
    "\\end{align}\n",
    "\n",
    "where we note that the first part of the expression is equal to eq. (9), so we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{e^{w_{k}^T x^n}}{\\sum_{k'}^{K} e^{w_{k'}^T x^n}} x_{j}^{n} \\sum_{k=1}^{K} y_{k}^{n} = \\hat{y}_{k}^{n} x_{j}^{n} \\sum_{k=1}^{K} y_{k}^{n}. \\tag{16}\n",
    "\\end{align}\n",
    "\n",
    "and finally, using eq. (10) we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{y}_{k}^{n} x_{j}^{n} \\sum_{k=1}^{K} y_{k}^{n} = \\hat{y}_{k}^{n} x_{j}^{n}. \\tag{17}\n",
    "\\end{align}\n",
    "\n",
    "Combining the two terms again, yields the gradient\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} &= - y_{k}^{n} x_{j}^{n} + \\hat{y}_{k}^{n} x_{j}^{n}, \\tag{18} \\\\\n",
    "    &= - x_{j}^{n} (y_{k}^{n} - \\hat{y}_{k}^{n}).\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](img/task2/task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](img/task2/task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping triggered at epoch: 19 and global step: 545"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "\n",
    "For this task I chose to reduce the number of epochs down to 50 and turn of early stopping to be able to see the spikes more clearly. In this case I would argue that it is difficult to see any difference between the level of spikes with and without. The reason that validation accuracy has less spikes with the shuffle compared to without, is that when the data is shuffled, the models is trained on a more generalized data set. This in turn reduces the risk of overfitting the model, which could result in more spikes.\n",
    "\n",
    "\n",
    "![](img/task2/task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](img/task3/task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](img/task3/task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Based on the Figures in Task 3b) and Task 3c) there is no appearant sign of overfitting. For the training and validation loss in the Figure in Task 3b) they seem to converge to a similar result, indicating that the training set represented the validation set sufficiently. However, it may be that the validation set is similar to the training set and not the original distribution of the data. Looking at the training and validationa accuracy however, the difference in accuracy between the two sets seems to become smaller and smaller. While this is not a sound proof that the model is overfitting, it is an indication that when the validation accuracy starts to deteroriate, the model is overfitted on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "From the task description, we have that\n",
    "\n",
    "\\begin{equation}\n",
    "    J(w) = C(w) + \\lambda R(w), \\quad R(w) = ||w||^2 = \\frac{1}{2} \\sum_{i, j} w_{i, j}^{2}, \\tag{19}\n",
    "\\end{equation}\n",
    "\n",
    "where $C(w)$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N} \\sum_{n = 1}^{N} C^{n}(w), \\quad C^{n}(w) = - \\sum_{k=1}^{K} y_{k}^{n} ln(\\hat{y}_{k}^{n}). \\tag{20}\n",
    "\\end{equation}\n",
    "\n",
    "Hence, the new gradient becomes\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J(w)}{\\partial w_{kj}} &= \\frac{1}{N} \\sum_{n = 1}^{N} \\frac{\\partial C^{n}(w)}{\\partial w_{kj}} + \\lambda \\frac{\\partial R(w)}{\\partial w_{kj}}, \\tag{20} \\\\\n",
    "    &=  \\frac{1}{N} \\sum_{n = 1}^{N} x_{j}^{n} (y_{k}^{n} - \\hat{y}_{j}^{n}) + \\lambda w_{kj}.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "The Figure below shows the weights with (bottom) and without (top) regularization. It is clear that the weights for the model with regularization, $\\lambda = 2.0$, is less noisy and more crisp. The reason for this is that the weights in the model with $\\lambda = 2.0$ are penalized more when they're too big, resulting in smaller weights. Whereas for the model without regularization we see that the contrast is higher between the digits due to the larger variation in absolute and relative weight sizes.\n",
    "\n",
    "![](img/task4/task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](img/task4/task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "The reason that the valiation accuracy degrades when applying any amount of regularization is that the underlying model only consists of one layer and has a relatively simple structure. Any increased complexity in hte trained model to increase validation accuracy will therefore be penalized resulting in lower accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "The Figure below confirms what we argued in Task 4d), that for an increasing lambda, the average values of the weights decreases, which in turn results in relatively simpler models comapred to without regularization.\n",
    "\n",
    "![](img/task4/task4d_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
