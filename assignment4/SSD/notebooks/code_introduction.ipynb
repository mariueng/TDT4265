{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e9740",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f495221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd())) # Include ../SSD in path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cd486",
   "metadata": {},
   "source": [
    "# Introduction to SSD\n",
    "\n",
    "This code is much more complex than your prior assignment, and we recommend you to spend some time getting familiar with the code structure.\n",
    "The \"complex\" code structure is made to simplify aspects of deep learning experimentation, and help you structure the usual \"sphagetti\" deep learning code.\n",
    "\n",
    "\n",
    "All scripts in this code requires a configuration file. To **start training**, you can type:\n",
    "```\n",
    "python train.py configs/ssd300.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8eadf",
   "metadata": {},
   "source": [
    "## Configuration files\n",
    "The key difference from previous starter codes is the use of configuration files. This enables us to change small parts of the experiment without changing hard-coded values (e.g. learning rate in previous assignments).\n",
    "\n",
    "If you take a look in [`configs/ssd300.py`](../configs/ssd300.py) you will notice a large set of objects describing model architecture (`backbone` and `model`), the optimizer, dataset, data loading, and hyperparameters.\n",
    "\n",
    "To load the config we can write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1590a924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are currently working on the server, with a 'working directory' in: /work/snotra/mariueng\n",
      "Saving all SSD outputs to: /work/snotra/mariueng/ssd_outputs\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/train\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/val\n"
     ]
    }
   ],
   "source": [
    "from ssd.utils import load_config\n",
    "cfg = load_config(\"../configs/ssd300.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a6774",
   "metadata": {},
   "source": [
    "`cfg` supports access syntax, where all objects in `configs/ssd300.py` are accessible via their attribute name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc040e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_extractor': {'output_channels': [128, 256, 128, 128, 64, 64], 'image_channels': '${train.image_channels}', 'output_feature_sizes': '${anchors.feature_sizes}', '_target_': <class 'ssd.modeling.backbones.basic.BasicModel'>}, 'anchors': {'feature_sizes': [[38, 38], [19, 19], [10, 10], [5, 5], [3, 3], [1, 1]], 'strides': [[8, 8], [16, 16], [32, 32], [64, 64], [100, 100], [300, 300]], 'min_sizes': [[30, 30], [60, 60], [111, 111], [162, 162], [213, 213], [264, 264], [315, 315]], 'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]], 'image_shape': '${train.imshape}', 'scale_center_variance': 0.1, 'scale_size_variance': 0.2, '_target_': <class 'ssd.modeling.anchor_boxes.AnchorBoxes'>}, 'loss_objective': <class 'ssd.modeling.ssd_multibox_loss.SSDMultiboxLoss'>, 'num_classes': 11, '_target_': <class 'ssd.modeling.ssd.SSD300'>}\n"
     ]
    }
   ],
   "source": [
    "print(cfg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de43d3e",
   "metadata": {},
   "source": [
    "If we print `cfg.model`, notice that it returns a dictionary and not the model object itself (which is `SSD300` from [`ssd/modeling/ssd.py`](../ssd/modeling/ssd.py)). This is because the model is defined \"lazily\" (wrapped with a `LazyCall`).\n",
    "\n",
    "To create the model, we have to instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7772c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSD300(\n",
      "  (feature_extractor): BasicModel(\n",
      "    (feature_map_zero): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): ReLU()\n",
      "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU()\n",
      "      (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (feature_map_one): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (feature_map_two): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (feature_map_three): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (feature_map_four): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (feature_map_five): Sequential(\n",
      "      (0): ReLU()\n",
      "      (1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): ReLU()\n",
      "      (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (loss_func): SSDMultiboxLoss(\n",
      "    (sl1_loss): SmoothL1Loss()\n",
      "    (CEL): CrossEntropyLoss()\n",
      "  )\n",
      "  (regression_heads): ModuleList(\n",
      "    (0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(128, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(128, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (classification_heads): ModuleList(\n",
      "    (0): Conv2d(128, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Conv2d(256, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): Conv2d(128, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Conv2d(128, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): Conv2d(64, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): Conv2d(64, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from tops.config import instantiate\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ec64d",
   "metadata": {},
   "source": [
    "Another example, we can load the first batch of the dataset and run a forward pass with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98b3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "(47040000,)\n",
      "(7840000,)\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image has the shape: torch.Size([32, 3, 300, 300])\n",
      "boxes has the shape: torch.Size([32, 8732, 4])\n",
      "labels has the shape: torch.Size([32, 8732])\n",
      "The model predicted anchors with  bbox delta: torch.Size([32, 4, 8732]) and confidences: torch.Size([32, 11, 8732])\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(type(dataloader_train))\n",
    "batch = next(iter(dataloader_train))\n",
    "for key, item in batch.items():\n",
    "    print(key, \"has the shape:\", item.shape)\n",
    "gpu_transform = instantiate(cfg.data_train.gpu_transform)\n",
    "batch = gpu_transform(batch)\n",
    "bbox_delta, confidences = model(batch[\"image\"])\n",
    "print(f\"The model predicted anchors with  bbox delta: {bbox_delta.shape} and confidences: {confidences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a3aa6",
   "metadata": {},
   "source": [
    "You might ask yourself, why? At first sight, this seems very complicated rather than plain-old  hard coded values.\n",
    "\n",
    "The reason is easy manipulation of experiments. If I want to run the same experiment, but with a different batch size, I can change it with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68cc47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original batch size: 32\n",
      "New batch size: 2\n"
     ]
    }
   ],
   "source": [
    "# Lets print the batch size of the original data loader:\n",
    "print(\"Original batch size:\", dataloader_train.batch_size)\n",
    "cfg.train.batch_size = 2 # Setting the batch size to 2\n",
    "dataloader_train = instantiate(cfg.data_train.dataloader)\n",
    "print(\"New batch size:\", dataloader_train.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe77ef",
   "metadata": {},
   "source": [
    "Another reason is **configuration inheritance**. E.g. for the last task, you are going to train VGG on the VOC dataset. \n",
    "This requires us to change the backbone and dataset, however, keep all other parameters the same (e.g. general configs (cfg.train), anchors, schedulers, optimizers, etc.).\n",
    "\n",
    "Take a look in [`configs/voc_vgg.py`](../configs/voc_vgg.py) and notice that we inherit from the original config:\n",
    "```\n",
    "from .ssd300 import train, anchors, optimizer, schedulers, model, data_train, data_val\n",
    "```\n",
    "The only changes done are to the backbone, dataset and dataset transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e8b0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are currently working on the server, with a 'working directory' in: /work/snotra/mariueng\n",
      "Saving all SSD outputs to: /work/snotra/mariueng/ssd_outputs\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/train\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/val\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2012\n",
      "Found dataset directory in: /work/datasets/VOCdevkit/VOC2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/mariueng/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m load_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../configs/voc_vgg.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/TDT4265/assignment4/SSD/tops/config/instantiate.py:60\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [instantiate(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m cfg]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg, abc\u001b[38;5;241m.\u001b[39mMapping) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cfg:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# conceptually equivalent to hydra.utils.instantiate(cfg) with _convert_=all,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# but faster: https://github.com/facebookresearch/hydra/issues/1200\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m {k: instantiate(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m instantiate(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/TDT4265/assignment4/SSD/tops/config/instantiate.py:60\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [instantiate(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m cfg]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg, abc\u001b[38;5;241m.\u001b[39mMapping) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cfg:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# conceptually equivalent to hydra.utils.instantiate(cfg) with _convert_=all,\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# but faster: https://github.com/facebookresearch/hydra/issues/1200\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m {k: \u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m instantiate(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/TDT4265/assignment4/SSD/tops/config/instantiate.py:76\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(cfg, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m callable(\u001b[38;5;28mcls\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_target_ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not define a callable object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/TDT4265/assignment4/SSD/ssd/modeling/backbones/vgg.py:27\u001b[0m, in \u001b[0;36mVGG.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvgg[\u001b[38;5;241m16\u001b[39m]\u001b[38;5;241m.\u001b[39mceil_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextras \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m     31\u001b[0m         nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     32\u001b[0m             nn\u001b[38;5;241m.\u001b[39mMaxPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,dilation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m         ),\n\u001b[1;32m     62\u001b[0m     ])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/vgg.py:168\u001b[0m, in \u001b[0;36mvgg16\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvgg16\u001b[39m(pretrained: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VGG:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"VGG 16-layer model (configuration \"D\")\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    The required minimum input size of the model is 32x32.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m        progress (bool): If True, displays a progress bar of the download to stderr\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvgg16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/vgg.py:106\u001b[0m, in \u001b[0;36m_vgg\u001b[0;34m(arch, cfg, batch_norm, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n\u001b[0;32m--> 106\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_urls\u001b[49m\u001b[43m[\u001b[49m\u001b[43march\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py:591\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[1;32m    589\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    590\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/hub.py:476\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hash_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m     sha256 \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256()\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m          \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit_divisor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    479\u001b[0m         buffer \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m8192\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py:242\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    241\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py:115\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIProgress not found. Please update jupyter and ipywidgets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m See https://ipywidgets.readthedocs.io/en/stable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/user_install.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[1;32m    120\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "cfg = load_config(\"../configs/voc_vgg.py\")\n",
    "model = instantiate(cfg.model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d6eca",
   "metadata": {},
   "source": [
    "# Useful commands:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1edf8c",
   "metadata": {},
   "source": [
    "#### Training and evaluation\n",
    "To start training:\n",
    "```\n",
    "python train.py  configs/ssd300.py\n",
    "```\n",
    "\n",
    "To starting training VGG on VOC:\n",
    "```\n",
    "python train.py  configs/voc_vgg.py\n",
    "```\n",
    "\n",
    "To only run evaluation:\n",
    "```\n",
    "python train.py  configs/ssd300.py --evaluate-only\n",
    "```\n",
    "\n",
    "#### Demo.py\n",
    "For VOC:\n",
    "```\n",
    "python demo.py configs/voc_vgg.py demo/voc demo/voc_output\n",
    "```\n",
    "\n",
    "For MNIST:\n",
    "```\n",
    "python demo.py configs/ssd300.py demo/mnist demo/mnist_output\n",
    "```\n",
    "\n",
    "\n",
    "#### Runtime analysis:\n",
    "```\n",
    "python3 runtime_analysis.py configs/ssd300.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
